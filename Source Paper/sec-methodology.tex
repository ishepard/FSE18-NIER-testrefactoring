%!TEX root = ./main.tex

\section{Methodology}
In this paper we aim to understand what type of refactorings developers apply the most on test code, as well as identifying the effects of those refactorings on production code. To this aim, we analyzed 3 open-source projects using a variety of available tools. In this section we elaborate on the chosen data-sources, research tools and the research questions we attempt to answer.

\subsection{Research Questions}
\label{rqs}
Below we will describe the research questions we will be answering in this paper.\\
\indent\textbf{RQ1} \textit{What type of refactorings do developers apply on test code?}\\
We want to identify what kind of refactoring methods developers apply to test code and see if we find any relation to the methods described in \cite{van2001refactoring}. We also want to look into the possible correlations to refactor methods applied to different types of system components.\\
\indent\textbf{RQ2} \textit{What is the relation between test code maintainability and production code maintainability?}\\
We aim to see whether there is a relation between test code maintainability and production code maintainability, so if an improvement or deterioration in test code maintainability also affects the production code. 

\subsection{Projects under investigation}
As subject systems for our study we consider 3 OSS projects and their 667 releases. The selection is driven by two main factors: firstly, since we have to run static analysis tools to different test refactorings and compute maintainability metrics, we focus on projects whose source code is publicly available (i.e., OSS); secondly, we analyze systems having a big corpus of test code. After filtering on these criteria, we randomly select 3 OSS projects from the list available on GitHub~\footnote{\url{https://github.com}} having different size, a big amount of releases and with a number of JUnit test cases higher than 1,000 in all the releases.

The used projects can be found in Table~\ref{table:1}. The repositories exists out of multiple projects which are either maven or gradle projects, containing a production and a test code directory. This project setup makes it more easier to find production/test pair files, which is very important for this research. All projects have a time span of at least 5 years and have over 150 releases, which means that there is a strong possibility that multiple refactors have been done in order to maintain the code.


\begin{table}[htb]
    \caption{An overview of the analysed projects}
    \label{table:1}
    \resizebox{0.8\columnwidth}{!}{%
    \begin{tabular}{lrrr} 
     \hline 
     \textbf{Project name} & \thead{\# of prod.\\files} & \thead{\# of test\\files} & \thead{\# of\\releases} \\
     \hline
     SonarQube & 3166 & 2085 & 165 \\
     Apache Hadoop & 5880 & 2498 & 288 \\ 
     ElasticSearch & 3667 & 1328 & 214 \\
     \hline
    \end{tabular}
    }
\end{table}

\subsection{Defining Maintainability}
\label{sec:maint-metric}
Since the notion of maintainability is not defined in literature, in this work we take into consideration previous studies' definitions and we create our own version of maintainability. 
On first sight one might be tempted to use the maintainability index~\footnote{\url{https://blogs.msdn.microsoft.com/zainnab/2011/05/26/code-metrics-maintainability-index/}}, however, previous studies~\cite{sjoberg2012questioning, heitlager2007practical} have demonstrated that the metric has some unreliable properties. The maintainability index can be a tool to help a developer improve his/her code, but can not be used to conclude if a project/class has a high maintainability or not. 

Research has showed \cite{sjoberg2012questioning} that the most reliable metrics of maintainability are the size of the project and the inverse cohesion. Metrics as \textbf{LOC} (Lines of Code), \textbf{NOF} (Number of Fields) and \textbf{NOM} (Number of Methods) all give some indication about the size of a class and are thereby more reliable for determining the maintainability. As in previous studies~\cite{citationneeded}, to create our version of maintainability index we also take into consideration the complexity of a class (\textbf{WMC}).

At the end, our own maintainability analysis method is based on the LOC, NOF, WMC and NOM of a Java file. Each metric is divided into 4 categories: ``Very Low'', ``Low'', ``Medium'' and ``High'', representing the risk of being poorly maintainable. As done in previous studies~\cite{alves2010deriving}, we base the boundaries of these categories on our own dataset, by taking the 70th, 80th and 90th quantile.

% \begin{table}[!ht]
%     \centering
%     \label{categories-LOC}
%     \begin{tabular}{|l|l|}
%         \hline
%         \textbf{Condition} & \textbf{Category} \\
%         \hline
%         x < 126 & Very Low \\
%         126 < x < 173 & Low \\
%         173 < x < 281 & Medium \\
%         281 < x & High \\
%         \hline
%     \end{tabular}
%     \caption{LOC maintainability categories}
% \end{table}
% \begin{table}[!ht]
%     \centering
%     \label{categories-NOF}
%     \begin{tabular}{|l|l|}
%         \hline
%         \textbf{Condition} & \textbf{Category} \\
%         \hline
%         x < 3 & Very Low \\
%         3 < x < 5 & Low \\
%         5 < x < 9 & Medium \\
%         9 < x & High \\
%         \hline
%     \end{tabular}
%     \caption{NOF maintainability categories}
% \end{table}
% \begin{table}[!ht]
%     \centering
%     \label{categories-WMC}
%     \begin{tabular}{|l|l|}
%         \hline
%         \textbf{Condition} & \textbf{Category} \\
%         \hline
%         x < 17 & Very Low \\
%         17 < x < 26 & Low \\
%         26 < x < 46 & Medium \\
%         46 < x & High \\
%         \hline
%     \end{tabular}
%     \caption{WMC maintainability categories}
% \end{table}
% \begin{table}[!ht]
%     \centering
%     \label{categories-NOM}
%     \begin{tabular}{|l|l|}
%         \hline
%         \textbf{Condition} & \textbf{Category} \\
%         \hline
%         x < 9 & Very Low \\
%         9 < x < 13 & Low \\
%         13 < x < 20 & Medium \\
%         20 < x & High \\
%         \hline
%     \end{tabular}
%     \caption{NOM maintainability categories}
% \end{table}

\subsection{Data Extraction}
\label{data-extraction}
To answer our research questions, we extracted information regarding the maintainability of a project and the types of refactorings developers apply the most on test code. To this aim, we use 3 OSS tools (they all can be found on GitHub): 
\begin{itemize}
    \item \textbf{Repodriller}: Java framework that allows the extraction of information such as commits, modifications, diffs, and source code. We used it to mine the change history information of the subject systems
    \item \textbf{CK}: Java tool to extract code metrics of Java systems
    \item \textbf{Refactoring-miner}: library written in Java that can detect refactorings applied in the history of a Java project.
\end{itemize}

To answer RQ$_1$, namely what type of refactorings developers apply the most on test code, we run the tool Refactoring-miner on all the commits of the subject systems on the last 5 years. We decided to narrow the total amount of considered commits since this process is highly time consuming. 
% To answer RQ$_2$, we collect information regarding the impact of test code refactoring on production code. For every commit of the last 5 years, we first check if there are actually any refactors made on test code: If this is not the case we skip the whole commit because it does not contain relevant information for this research question. Instead, if the commit does contain refactors, for every refactored test file we fetch the matching production class and calculate the metrics of this file every 10 commits for 5 versions (\ie we calculate the metrics of the file after 10, 20, 30, 40, 50 commits) after the test refactoring had taken place. To match the production class we exploit a traceability technique based on naming convention, \ie, it identifies the methods under test by removing the string ‘Test’ from the method name of the JUnit test method. This technique has been previously evaluated by Sneed~\cite{sneed2004reverse}, demonstrating the highest performance (both in terms of accuracy and scalability) with respect to other traceability approaches (e.g., slicing-based approaches~\cite{qusef2014recovering}). 
To answer RQ$_2$ and study the relation between test and production code maintainability, we collect monthly metrics of the systems and then map every production file to its test file. To correctly match the two types of files, we exploit a traceability technique based on naming convention, \ie, it identifies the methods under test by removing the string ‘Test’ from the method name of the JUnit test method. This technique has been previously evaluated by Sneed~\cite{sneed2004reverse}, demonstrating the highest performance (both in terms of accuracy and scalability) with respect to other traceability approaches (e.g., slicing-based approaches~\cite{qusef2014recovering}). 
For this research question, we decided to consider all the history of the projects (instead of only 5 years as previously done for RQ$_1$), but on a monthly base. More specifically, for all the subject systems, we monthly checkout the code base and collect the metrics of the entire system using \emph{CK}. 

The details about the data extraction can be found in Table.~\ref{table:6}.

\begin{table}[htb]
\caption{Statistics of the 3 projects.}
\label{table:6}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lrrr}
 & Sonarqube & Hadoop & Elasticsearch \\
\hline
\begin{tabular}[c]{@{}l@{}}Amount of monthly\\metrics commits\end{tabular} & 55 & 77 & 45 \\ 
\hline
\begin{tabular}[c]{@{}l@{}}Average amount of\\evaluated Java files\end{tabular} & 3170 & 1466 & 3875 \\ 
\hline
\begin{tabular}[c]{@{}l@{}}Total amount of\\test refactors\end{tabular} & 6832 & 5452 & 10933\\
\hline
\end{tabular}
}
\end{table}
